{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53ee5a96-0bc3-4f81-a52a-221c0090ae6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:43:17.499717: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 14:43:17.588430: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-28 14:43:17.608446: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-28 14:43:18.063325: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 14:43:18.063392: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 14:43:18.063398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66102afa-0c7b-45ba-ae5e-a56f40b270af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpython\u001b[49m \u001b[38;5;241m-\u001b[39mV\n",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": [
    "python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe4e2fc9-bcf8-4193-8768-cdae2342141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:43:22.900928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:22.922001: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:22.922355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:22.922888: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 14:43:22.923891: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:22.924200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:22.924542: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:23.498621: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:23.499012: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:23.499024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-09-28 14:43:23.499255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:43:23.499287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21614 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(np.random.rand(3000,3000))\n",
    "b = tf.Variable(np.random.rand(3000,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523def52-7218-4dac-9465-47095e502c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131 µs ± 73.9 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "c = tf.matmul(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11a83e5-1510-4acf-ad5b-a3f4f3e48d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(3000,3000)\n",
    "b = np.random.rand(3000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02bee8d9-7a16-4e68-8c77-f88b460c7710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145 ms ± 3.4 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "c = np.dot(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b05fdf43-bebf-43b4-a02b-802981b5ba38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 14:16:50.854169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:16:50.928191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 14:16:50.928432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b2c4b34-d863-46cf-8dea-67c63bfe5774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 15:02:15.831449: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 15:02:15.916755: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-28 15:02:15.936984: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-28 15:02:16.338806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 15:02:16.338864: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 15:02:16.338869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-09-28 15:02:17.061977: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 15:02:17.082595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-28 15:02:17.083025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a443d3b-0aa0-4ca7-9376-ab4d89a047b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-28 15:17:41.006937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-28 15:17:41.099210: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-28 15:17:41.119826: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-28 15:17:41.602268: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 15:17:41.602334: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-28 15:17:41.602340: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "457b8e1e-2975-4e81-b210-ea919abc906e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ffab41-d9fd-4205-a0be-39d5e8237109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a112842-4637-49b5-b46d-314b4bdd1972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 18:48:27.038804: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-29 18:48:27.135563: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-09-29 18:48:27.156803: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-29 18:48:27.551044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-29 18:48:27.551111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-29 18:48:27.551117: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2022-09-29 18:48:29.219285: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.239801: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.240089: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.240701: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-29 18:48:29.241757: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.242088: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.242382: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.952471: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.953019: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.953041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-09-29 18:48:29.953417: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-09-29 18:48:29.953460: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21614 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25630\n",
      "RESNET50\n",
      "64\n",
      "64\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-29 18:50:05.634300: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-09-29 18:50:06.198283: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2022-09-29 18:50:06.427331: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0001.cpkt\n",
      "321/321 - 458s - loss: 0.0120 - val_loss: 0.0056 - 458s/epoch - 1s/step\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0002.cpkt\n",
      "321/321 - 421s - loss: 0.0092 - val_loss: 0.0086 - 421s/epoch - 1s/step\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0003.cpkt\n",
      "321/321 - 416s - loss: 0.0047 - val_loss: 0.0014 - 416s/epoch - 1s/step\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0004.cpkt\n",
      "321/321 - 415s - loss: 0.0276 - val_loss: 0.0102 - 415s/epoch - 1s/step\n",
      "Epoch 5/20\n",
      "\n",
      "Epoch 5: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0005.cpkt\n",
      "321/321 - 414s - loss: 0.0034 - val_loss: 0.0027 - 414s/epoch - 1s/step\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0006.cpkt\n",
      "321/321 - 422s - loss: 0.0029 - val_loss: 0.0049 - 422s/epoch - 1s/step\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0007.cpkt\n",
      "321/321 - 412s - loss: 0.0037 - val_loss: 4.8086e-04 - 412s/epoch - 1s/step\n",
      "Epoch 8/20\n",
      "\n",
      "Epoch 8: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0008.cpkt\n",
      "321/321 - 414s - loss: 0.0010 - val_loss: 7.3600e-04 - 414s/epoch - 1s/step\n",
      "Epoch 9/20\n",
      "\n",
      "Epoch 9: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0009.cpkt\n",
      "321/321 - 417s - loss: 5.1974e-04 - val_loss: 2.1831e-04 - 417s/epoch - 1s/step\n",
      "Epoch 10/20\n",
      "\n",
      "Epoch 10: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0010.cpkt\n",
      "321/321 - 419s - loss: 9.8317e-04 - val_loss: 1.0470e-04 - 419s/epoch - 1s/step\n",
      "Epoch 11/20\n",
      "\n",
      "Epoch 11: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0011.cpkt\n",
      "321/321 - 425s - loss: 5.5296e-04 - val_loss: 0.0047 - 425s/epoch - 1s/step\n",
      "Epoch 12/20\n",
      "\n",
      "Epoch 12: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0012.cpkt\n",
      "321/321 - 413s - loss: 0.0134 - val_loss: 0.0024 - 413s/epoch - 1s/step\n",
      "Epoch 13/20\n",
      "\n",
      "Epoch 13: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0013.cpkt\n",
      "321/321 - 414s - loss: 0.0151 - val_loss: 0.0015 - 414s/epoch - 1s/step\n",
      "Epoch 14/20\n",
      "\n",
      "Epoch 14: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0014.cpkt\n",
      "321/321 - 425s - loss: 9.8335e-04 - val_loss: 3.5766e-04 - 425s/epoch - 1s/step\n",
      "Epoch 15/20\n",
      "\n",
      "Epoch 15: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0015.cpkt\n",
      "321/321 - 413s - loss: 5.5368e-04 - val_loss: 7.2903e-04 - 413s/epoch - 1s/step\n",
      "Epoch 16/20\n",
      "\n",
      "Epoch 16: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0016.cpkt\n",
      "321/321 - 413s - loss: 6.5804e-04 - val_loss: 0.0013 - 413s/epoch - 1s/step\n",
      "Epoch 17/20\n",
      "\n",
      "Epoch 17: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0017.cpkt\n",
      "321/321 - 412s - loss: 9.8209e-04 - val_loss: 0.0015 - 412s/epoch - 1s/step\n",
      "Epoch 18/20\n",
      "\n",
      "Epoch 18: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0018.cpkt\n",
      "321/321 - 413s - loss: 2.9906e-04 - val_loss: 0.0018 - 413s/epoch - 1s/step\n",
      "Epoch 19/20\n",
      "\n",
      "Epoch 19: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0019.cpkt\n",
      "321/321 - 415s - loss: 0.0029 - val_loss: 0.0028 - 415s/epoch - 1s/step\n",
      "Epoch 20/20\n",
      "\n",
      "Epoch 20: saving model to gs://cards-data/yugioh/models/RESNET50_64/cp-0020.cpkt\n",
      "321/321 - 414s - loss: 0.0011 - val_loss: 6.2475e-04 - 414s/epoch - 1s/step\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'gs://cards-data/yugioh/embeddings/anchor_embedding_RESNET50_64.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m m\u001b[38;5;241m.\u001b[39mcreate_model()\n\u001b[1;32m      7\u001b[0m m\u001b[38;5;241m.\u001b[39mtrain_model(epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_anchor_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/nmagh/Desktop/projects/YGOSiamese/train.py:174\u001b[0m, in \u001b[0;36mTrainModel.fetch_anchor_embeddings\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings/anchor_embedding_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpretrained_model\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m    172\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_vec, outfile)\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgs://cards-data/yugioh/embeddings/anchor_embedding_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m outfile:\n\u001b[1;32m    175\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_vec, outfile)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gs://cards-data/yugioh/embeddings/anchor_embedding_RESNET50_64.json'"
     ]
    }
   ],
   "source": [
    "from train import TrainModel\n",
    "m = TrainModel(pretrained_model='RESNET50', )\n",
    "m.process_file_data()\n",
    "m.create_dataset()\n",
    "m.create_train_val_dataset()\n",
    "m.create_model()\n",
    "m.train_model(epochs=20)\n",
    "m.fetch_anchor_embeddings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64a64b59-f6d7-4974-865b-c2a63be2f4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "DS\n",
      "<tf.Variable 'Variable:0' shape=(1, 224, 224, 3) dtype=float32, numpy=\n",
      "array([[[[0.46089232, 0.5327569 , 0.6922046 ],\n",
      "         [0.4190387 , 0.49430305, 0.653908  ],\n",
      "         [0.4399537 , 0.519204  , 0.67842275],\n",
      "         ...,\n",
      "         [0.3315025 , 0.4574646 , 0.6176374 ],\n",
      "         [0.28118473, 0.35270882, 0.49923483],\n",
      "         [0.21535128, 0.27437264, 0.3721475 ]],\n",
      "\n",
      "        [[0.5779606 , 0.6654823 , 0.8522032 ],\n",
      "         [0.5656887 , 0.66126   , 0.8459501 ],\n",
      "         [0.57463247, 0.677728  , 0.85957074],\n",
      "         ...,\n",
      "         [0.3694898 , 0.49125162, 0.6828619 ],\n",
      "         [0.20785949, 0.28823215, 0.45216122],\n",
      "         [0.20301251, 0.26594704, 0.36687326]],\n",
      "\n",
      "        [[0.54618746, 0.66536826, 0.8699528 ],\n",
      "         [0.5603057 , 0.6851127 , 0.8870492 ],\n",
      "         [0.56414473, 0.6885823 , 0.90000045],\n",
      "         ...,\n",
      "         [0.38934067, 0.51930666, 0.72186124],\n",
      "         [0.20000017, 0.28684038, 0.4612758 ],\n",
      "         [0.19331786, 0.2587934 , 0.36073524]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.02153712, 0.20884764, 0.544702  ],\n",
      "         [0.02615837, 0.21117103, 0.56726444],\n",
      "         [0.02209572, 0.20437017, 0.58410907],\n",
      "         ...,\n",
      "         [0.33590364, 0.6187596 , 0.7624216 ],\n",
      "         [0.1422504 , 0.30315843, 0.44573295],\n",
      "         [0.14650598, 0.28876117, 0.39837322]],\n",
      "\n",
      "        [[0.02634305, 0.21152462, 0.50277036],\n",
      "         [0.04353729, 0.22585171, 0.5328392 ],\n",
      "         [0.03139584, 0.21031292, 0.54179174],\n",
      "         ...,\n",
      "         [0.32144254, 0.57356614, 0.7176103 ],\n",
      "         [0.17859289, 0.31205037, 0.44998208],\n",
      "         [0.17723632, 0.30047014, 0.40373933]],\n",
      "\n",
      "        [[0.15213263, 0.29125634, 0.44716117],\n",
      "         [0.16637866, 0.30286974, 0.47200564],\n",
      "         [0.16906297, 0.30399677, 0.4941462 ],\n",
      "         ...,\n",
      "         [0.19684634, 0.40647113, 0.55171484],\n",
      "         [0.2002119 , 0.29506165, 0.4353722 ],\n",
      "         [0.21505941, 0.30758524, 0.40950876]]]], dtype=float32)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_anchor_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/nmagh/Desktop/projects/YGOSiamese/train.py:163\u001b[0m, in \u001b[0;36mTrainModel.fetch_anchor_embeddings\u001b[0;34m(self, save)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m : \n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mANCHOR_EMBEDDINGS\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_and_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    164\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m############################\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# pool = mp.Pool(num_cores)\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# ANCHOR_EMBEDDINGS = pool.map(preprocess_and_embed, tmp)\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/nmagh/Desktop/projects/YGOSiamese/train.py:152\u001b[0m, in \u001b[0;36mTrainModel.preprocess_and_embed\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDS\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[0;32m--> 152\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPREPROCESS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/resnet_v2.py:137\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.resnet_v2.preprocess_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:123\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _preprocess_numpy_input(x, data_format\u001b[38;5;241m=\u001b[39mdata_format, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_preprocess_symbolic_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:271\u001b[0m, in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"Preprocesses a tensor encoding a batch of images.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    Preprocessed tensor.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m127.5\u001b[39m\n\u001b[1;32m    272\u001b[0m     x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1494\u001b[0m, in \u001b[0;36mBaseResourceVariable.__itruediv__\u001b[0;34m(self, unused_other)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__itruediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unused_other):\n\u001b[0;32m-> 1494\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`var /= value` with `tf.Variable`s is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported. Use `var.assign(var / value)` to modify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1496\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe variable, or `out = var / value` if you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1497\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to get a new output Tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor."
     ]
    }
   ],
   "source": [
    "m.fetch_anchor_embeddings(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d4315a-73fb-4c1f-8fd7-991d69dde7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "b'Images/regular_crop/1st_Movement_Solo/44256816.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49529f6e-4057-48d9-bf71-ca466d8a182d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MOBILENETV2'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.pretrained_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "608f355f-8de2-4bf6-b443-70e7e593ef81",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to function call (683365446.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [12]\u001b[0;36m\u001b[0m\n\u001b[0;31m    m.preprocess_image(b'Images/regular_crop/1st_Movement_Solo/44256816.jpg') /= 127.\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m cannot assign to function call\n"
     ]
    }
   ],
   "source": [
    "import model_references as MR\n",
    "m.preprocess_image(b'Images/regular_crop/1st_Movement_Solo/44256816.jpg') /= 127."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "147f9b32-8c34-4c16-ba33-7c8bedfa9f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.Variable(m.preprocess_image('Images/regular_crop/Dark_Alligator/34479658.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5eaf524-3f3b-41f2-bc55-e4559a0b4e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPREPROCESS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRESNET50V2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/resnet_v2.py:137\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.resnet_v2.preprocess_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:123\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _preprocess_numpy_input(x, data_format\u001b[38;5;241m=\u001b[39mdata_format, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_preprocess_symbolic_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:271\u001b[0m, in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"Preprocesses a tensor encoding a batch of images.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    Preprocessed tensor.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m127.5\u001b[39m\n\u001b[1;32m    272\u001b[0m     x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1494\u001b[0m, in \u001b[0;36mBaseResourceVariable.__itruediv__\u001b[0;34m(self, unused_other)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__itruediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unused_other):\n\u001b[0;32m-> 1494\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`var /= value` with `tf.Variable`s is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported. Use `var.assign(var / value)` to modify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1496\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe variable, or `out = var / value` if you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1497\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to get a new output Tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor."
     ]
    }
   ],
   "source": [
    "MR.PREPROCESS['RESNET50V2'].preprocess_input(tf.Variable(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2740d38d-e2f6-45c2-bef2-5cde41aaef9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "`var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPREPROCESS\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRESNET50V2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/resnet_v2.py:137\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.resnet_v2.preprocess_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:123\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _preprocess_numpy_input(x, data_format\u001b[38;5;241m=\u001b[39mdata_format, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_preprocess_symbolic_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:271\u001b[0m, in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"Preprocesses a tensor encoding a batch of images.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    Preprocessed tensor.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m127.5\u001b[39m\n\u001b[1;32m    272\u001b[0m     x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1494\u001b[0m, in \u001b[0;36mBaseResourceVariable.__itruediv__\u001b[0;34m(self, unused_other)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__itruediv__\u001b[39m(\u001b[38;5;28mself\u001b[39m, unused_other):\n\u001b[0;32m-> 1494\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`var /= value` with `tf.Variable`s is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1495\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupported. Use `var.assign(var / value)` to modify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1496\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe variable, or `out = var / value` if you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1497\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneed to get a new output Tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `var /= value` with `tf.Variable`s is not supported. Use `var.assign(var / value)` to modify the variable, or `out = var / value` if you need to get a new output Tensor."
     ]
    }
   ],
   "source": [
    "MR.PREPROCESS['RESNET50V2'].preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5701cc-ec49-44e3-9989-fcfd14554f43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73274142-72a3-4e62-91a6-e1e0ebc42165",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /=: 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mMR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPREPROCESS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mImages/regular_crop/1st_Movement_Solo/44256816.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/resnet_v2.py:137\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.applications.resnet_v2.preprocess_input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_input\u001b[39m(x, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimagenet_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_input\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:123\u001b[0m, in \u001b[0;36mpreprocess_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _preprocess_numpy_input(x, data_format\u001b[38;5;241m=\u001b[39mdata_format, mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_preprocess_symbolic_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/applications/imagenet_utils.py:271\u001b[0m, in \u001b[0;36m_preprocess_symbolic_input\u001b[0;34m(x, data_format, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;124;03m\"\"\"Preprocesses a tensor encoding a batch of images.\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;03m    Preprocessed tensor.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 271\u001b[0m     x \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m127.5\u001b[39m\n\u001b[1;32m    272\u001b[0m     x \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /=: 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "MR.PREPROCESS[m.pretrained_model].preprocess_input([m.preprocess_image(b'Images/regular_crop/1st_Movement_Solo/44256816.jpg')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81434fff-dcb0-4c73-bdd5-b7c0a0bab64b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
